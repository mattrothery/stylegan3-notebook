{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mattrothery/stylegan3-notebook/blob/main/Stylegan3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGd1izpBEOFe"
      },
      "source": [
        "# Stylegan3 Notebook\n",
        "\n",
        "Finetune premade models on a custom image dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RjeXFtljl5x",
        "outputId": "5c0e8620-14d8-4bcb-f096-58653ceb6e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# print(torch.__version__)\n",
        "# !pip uninstall torch -y\n",
        "# !pip install torch==1.9.0\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0PoOn9JJWKr",
        "outputId": "a94e4ec9-6e0f-468c-d23b-b9c3e5520abb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'stylegan3'...\n",
            "remote: Enumerating objects: 193, done.\u001b[K\n",
            "remote: Total 193 (delta 0), reused 0 (delta 0), pack-reused 193\u001b[K\n",
            "Receiving objects: 100% (193/193), 4.18 MiB | 17.74 MiB/s, done.\n",
            "Resolving deltas: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAvZF-u1_vm7",
        "outputId": "338dbf08-fd1f-4c3d-bd50-f361f87f55a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Jan 27 15:27:40 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHOIqOmmDY9t"
      },
      "source": [
        "# Training data tools\n",
        "\n",
        "Use the following code cells to unzip training images from Google Drive to a folder, remove any non-image files, then resize all to a set resolution. Must first mount drive in the files sidebar to the left. The training directory can then be passed to the dataset tool to put them in the right format for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3I2GQ5eWCkCM"
      },
      "outputs": [],
      "source": [
        "# # Unzip training images to a directory\n",
        "# !mkdir train\n",
        "# !unzip drive/MyDrive/train.zip -d train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca3enMp3CnEA"
      },
      "outputs": [],
      "source": [
        "# import os, sys\n",
        "\n",
        "# path = \"train/\"\n",
        "# dirs = os.listdir(path)\n",
        "\n",
        "# # Remove non-images\n",
        "# for fname in os.listdir('train/'):\n",
        "#     if not fname.endswith('.jpg'):\n",
        "#         os.remove(os.path.join(path, fname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMxwtxGaCo7d"
      },
      "outputs": [],
      "source": [
        "# # Resize images - may not need this as dataset_tool can do this for us\n",
        "# from PIL import Image\n",
        "# from PIL import ImageFile\n",
        "# ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# def resize():\n",
        "#     for item in dirs:\n",
        "#         if os.path.isfile(path + item):\n",
        "#             im = Image.open(path + item)\n",
        "#             if im.mode != 'RGB':\n",
        "#                 im = im.convert('RGB')\n",
        "#             fname, extension = os.path.splitext(path + item)\n",
        "#             imResize = im.resize((256, 256), Image.ANTIALIAS)\n",
        "#             imResize.save(fname + ' resized.jpg', 'JPEG', quality=90)\n",
        "\n",
        "# resize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UucRf8SCrLm"
      },
      "outputs": [],
      "source": [
        "# # Remove non-resized images\n",
        "# keyword = 'resized'\n",
        "# for fname in os.listdir('train/'):\n",
        "#     if keyword not in fname:\n",
        "#         os.remove(os.path.join(path, fname))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnaT-CkTkqQw",
        "outputId": "2b9313a0-81d7-4c96-9315-9a03f321147c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100% 15022/15022 [05:58<00:00, 41.91it/s] \n"
          ]
        }
      ],
      "source": [
        "# Create a training dataset from folder in Drive and set resolution\n",
        "!python stylegan3/dataset_tool.py --source drive/MyDrive/train256/ --dest train/ --resolution=256x256"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vyg0OLL0MDAN"
      },
      "outputs": [],
      "source": [
        "# Premade model .pkl file from NVidia\n",
        "# https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T0w1NJDHwLF",
        "outputId": "61715214-8b17-4715-b1e2-763a8b09eb9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[?25l\r\u001b[K     |███                             | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 20 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 30 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 40 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 51 kB 5.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 71 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 92 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 102 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 108 kB 4.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: ninja\n",
            "Successfully installed ninja-1.10.2.3\n"
          ]
        }
      ],
      "source": [
        "!pip install ninja"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ql-3i8h4FAq",
        "outputId": "d7188767-38bd-4553-ff17-edb6793b318a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan3.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 2\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 1024,\n",
            "    \"magnitude_ema_beta\": 0.9988915792636801,\n",
            "    \"conv_kernel\": 1,\n",
            "    \"use_radial_filters\": true\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks_stylegan2.Discriminator\",\n",
            "    \"block_kwargs\": {\n",
            "      \"freeze_layers\": 13\n",
            "    },\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 16384,\n",
            "    \"channel_max\": 512\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.0025\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08,\n",
            "    \"lr\": 0.002\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 0.5,\n",
            "    \"blur_init_sigma\": 0,\n",
            "    \"blur_fade_kimg\": 200.0\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"prefetch_factor\": 2,\n",
            "    \"num_workers\": 3\n",
            "  },\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"train/\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 15022,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256,\n",
            "    \"random_seed\": 0\n",
            "  },\n",
            "  \"num_gpus\": 1,\n",
            "  \"batch_size\": 32,\n",
            "  \"batch_gpu\": 16,\n",
            "  \"metrics\": [],\n",
            "  \"total_kimg\": 1,\n",
            "  \"kimg_per_tick\": 4,\n",
            "  \"image_snapshot_ticks\": 10,\n",
            "  \"network_snapshot_ticks\": 10,\n",
            "  \"random_seed\": 0,\n",
            "  \"ema_kimg\": 10.0,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1,\n",
            "    \"brightness\": 1,\n",
            "    \"contrast\": 1,\n",
            "    \"lumaflip\": 1,\n",
            "    \"hue\": 1,\n",
            "    \"saturation\": 1\n",
            "  },\n",
            "  \"ada_target\": 0.6,\n",
            "  \"resume_pkl\": \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"ema_rampup\": null,\n",
            "  \"run_dir\": \"training-runs/00000-stylegan3-r--gpus1-batch32-gamma0.5\"\n",
            "}\n",
            "\n",
            "Output directory:    training-runs/00000-stylegan3-r--gpus1-batch32-gamma0.5\n",
            "Number of GPUs:      1\n",
            "Batch size:          32 images\n",
            "Training duration:   1 kimg\n",
            "Dataset path:        train/\n",
            "Dataset size:        15022 images\n",
            "Dataset resolution:  256\n",
            "Dataset labels:      False\n",
            "Dataset x-flips:     True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 3 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "\n",
            "Num images:  30044\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "Resuming from \"https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl\"\n",
            "Downloading https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl ... done\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n",
            "\n",
            "Generator                     Parameters  Buffers  Output shape         Datatype\n",
            "---                           ---         ---      ---                  ---     \n",
            "mapping.fc0                   262656      -        [16, 512]            float32 \n",
            "mapping.fc1                   262656      -        [16, 512]            float32 \n",
            "mapping                       -           512      [16, 16, 512]        float32 \n",
            "synthesis.input.affine        2052        -        [16, 4]              float32 \n",
            "synthesis.input               1048576     3081     [16, 1024, 36, 36]   float32 \n",
            "synthesis.L0_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L0_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L1_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L1_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L2_36_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L2_36_1024          1049600     157      [16, 1024, 36, 36]   float32 \n",
            "synthesis.L3_52_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L3_52_1024          1049600     169      [16, 1024, 52, 52]   float16 \n",
            "synthesis.L4_52_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L4_52_1024          1049600     157      [16, 1024, 52, 52]   float16 \n",
            "synthesis.L5_84_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L5_84_1024          1049600     169      [16, 1024, 84, 84]   float16 \n",
            "synthesis.L6_84_1024.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L6_84_1024          1049600     157      [16, 1024, 84, 84]   float16 \n",
            "synthesis.L7_148_724.affine   525312      -        [16, 1024]           float32 \n",
            "synthesis.L7_148_724          742100      169      [16, 724, 148, 148]  float16 \n",
            "synthesis.L8_148_512.affine   371412      -        [16, 724]            float32 \n",
            "synthesis.L8_148_512          371200      157      [16, 512, 148, 148]  float16 \n",
            "synthesis.L9_148_362.affine   262656      -        [16, 512]            float32 \n",
            "synthesis.L9_148_362          185706      157      [16, 362, 148, 148]  float16 \n",
            "synthesis.L10_276_256.affine  185706      -        [16, 362]            float32 \n",
            "synthesis.L10_276_256         92928       169      [16, 256, 276, 276]  float16 \n",
            "synthesis.L11_276_181.affine  131328      -        [16, 256]            float32 \n",
            "synthesis.L11_276_181         46517       157      [16, 181, 276, 276]  float16 \n",
            "synthesis.L12_276_128.affine  92853       -        [16, 181]            float32 \n",
            "synthesis.L12_276_128         23296       25       [16, 128, 276, 276]  float16 \n",
            "synthesis.L13_256_128.affine  65664       -        [16, 128]            float32 \n",
            "synthesis.L13_256_128         16512       25       [16, 128, 256, 256]  float16 \n",
            "synthesis.L14_256_3.affine    65664       -        [16, 128]            float32 \n",
            "synthesis.L14_256_3           387         1        [16, 3, 256, 256]    float16 \n",
            "synthesis                     -           -        [16, 3, 256, 256]    float32 \n",
            "---                           ---         ---      ---                  ---     \n",
            "Total                         15779565    5576     -                    -       \n",
            "\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape         Datatype\n",
            "---            ---         ---      ---                  ---     \n",
            "b256.fromrgb   -           272      [16, 64, 256, 256]   float16 \n",
            "b256.skip      -           8208     [16, 128, 128, 128]  float16 \n",
            "b256.conv0     -           36944    [16, 64, 256, 256]   float16 \n",
            "b256.conv1     -           73872    [16, 128, 128, 128]  float16 \n",
            "b256           -           16       [16, 128, 128, 128]  float16 \n",
            "b128.skip      -           32784    [16, 256, 64, 64]    float16 \n",
            "b128.conv0     -           147600   [16, 128, 128, 128]  float16 \n",
            "b128.conv1     -           295184   [16, 256, 64, 64]    float16 \n",
            "b128           -           16       [16, 256, 64, 64]    float16 \n",
            "b64.skip       -           131088   [16, 512, 32, 32]    float16 \n",
            "b64.conv0      -           590096   [16, 256, 64, 64]    float16 \n",
            "b64.conv1      -           1180176  [16, 512, 32, 32]    float16 \n",
            "b64            -           16       [16, 512, 32, 32]    float16 \n",
            "b32.skip       -           262160   [16, 512, 16, 16]    float16 \n",
            "b32.conv0      -           2359824  [16, 512, 32, 32]    float16 \n",
            "b32.conv1      -           2359824  [16, 512, 16, 16]    float16 \n",
            "b32            -           16       [16, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [16, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \n",
            "b16            -           16       [16, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [16, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \n",
            "b8             -           16       [16, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [16, 512]            float32 \n",
            "b4.out         513         -        [16, 1]              float32 \n",
            "---            ---         ---      ---                  ---     \n",
            "Total          16523265    7478240  -                    -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 1 kimg...\n",
            "\n",
            "tick 0     kimg 0.0      time 6m 21s       sec/tick 30.2    sec/kimg 943.50  maintenance 351.1  cpumem 5.61   gpumem 9.55   reserved 10.47  augment 0.000\n",
            "tick 1     kimg 1.0      time 19m 18s      sec/tick 694.8   sec/kimg 700.38  maintenance 82.3   cpumem 5.66   gpumem 8.82   reserved 10.16  augment 0.010\n",
            "\n",
            "Exiting...\n"
          ]
        }
      ],
      "source": [
        "# Initiate training loop. As a test, kimg set to 1 \n",
        "!python stylegan3/train.py --outdir=training-runs/ --cfg=stylegan3-r --data=train/ --freezed=13 \\\n",
        "      --gpus=1 --batch=32 --gamma=0.5 --mirror=1 --kimg=1 --batch-gpu=16 --snap=10 --metrics=none --cbase=16384\\\n",
        "      --resume=https://api.ngc.nvidia.com/v2/models/nvidia/research/stylegan3/versions/1/files/stylegan3-r-ffhqu-256x256.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QsRMAU7xKLnM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d532f70-40f2-4f4d-9292-b8db7e5bc84d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"training-runs/00000-stylegan3-r--gpus1-batch32-gamma0.5/network-snapshot-000001.pkl\"...\n",
            "Generating image for seed 2 (0/1) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"filtered_lrelu_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "# Generate an image from seed number, using trained model .pkl file\n",
        "!python stylegan3/gen_images.py --outdir=out/ --trunc=1 --seeds=2 \\\n",
        "    --network=training-runs/00000-stylegan3-r--gpus1-batch32-gamma0.5/network-snapshot-000001.pkl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Stylegan3.ipynb",
      "provenance": [],
      "mount_file_id": "1jhdcrjUCI51mnButN3WFy0Q9GXuVUe-K",
      "authorship_tag": "ABX9TyN2fP7HQEfbBW9cWTkxBnj3",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}